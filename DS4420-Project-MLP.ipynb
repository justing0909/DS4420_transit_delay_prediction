{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d023b41-3653-40d9-a995-da2ee35e1d66",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e32326-cd14-4daf-8e36-e608110af237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f00c6-7dac-46f3-9387-de1d70243bf5",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174f379-57b8-4117-ba37-fa94410480f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596ea43-3342-4750-8021-261a1cb60f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58070d89-2653-415d-9963-c746d10f9717",
   "metadata": {},
   "source": [
    "# pointers\n",
    "(set pointers for x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f8aa3c02-26d3-4a78-90a1-907f5b342a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite when ready.....\n",
    "x_train = np.zeros((300,2))\n",
    "x_test  = np.zeros((100,2))\n",
    "y_train = np.zeros((300,1))\n",
    "y_test  = np.zeros((100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea9414-17ee-4091-9b5c-8f3f8225f902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38131b9d-305c-4b2d-b3b5-2d9d6a84f423",
   "metadata": {},
   "source": [
    "# define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "fd28f343-ffdc-4027-9fc8-43eb3f4a4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAF:\n",
    "    \"\"\"Abstract Activation Function.\n",
    "    This is an abstract class used to represent\n",
    "    activation functions in a MultiLayer Perceptitron.\n",
    "\n",
    "    Each Activation function must have a name\n",
    "    and implement the following three functions:\n",
    "    - fw(w,x)         represents a forward pass through the MLP\n",
    "    - bp(w,x)         represents a dL/dw backprop through the MLP\n",
    "    - bp_partial(w,x) represents a dL/dh backprop through the MLP <- TODO look into this.....\n",
    "\n",
    "    This class does not implement any of the three functions.\n",
    "    Child-classes MUST implement all three functions for \n",
    "    backprop to work properly.\n",
    "\n",
    "    In the current implementation, the following classes are the only valid subclasses:\n",
    "    - LinearAF\n",
    "    - ReluAF\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Abstract\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Overwrites the representation with class name.\n",
    "        This function makes the print look cleaner :) \n",
    "        \"\"\"\n",
    "        return f\"<ActivationFunction:{self.name}>\"\n",
    "    \n",
    "    def fw(self,w,x):\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "    def bp(self,w,x):\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "    def bp_partial(self,w,x):\n",
    "        # i'll double check the math on this one....\n",
    "        # from a preliminary check though, i think\n",
    "        # we might need this to run the backprop cleanly\n",
    "        # (with this sort of class setup)\n",
    "        # i think we'd have to chain partial-derivatives\n",
    "        # or something or other?\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "class LinearAF(AbstractAF):\n",
    "    \"\"\"Linear Activation Function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Linear\"\n",
    "    \n",
    "    def fw(self,w,x):\n",
    "        return w.T.dot(x)\n",
    "\n",
    "    def bp(self,w,x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class ReluAF(AbstractAF):\n",
    "    \"\"\"Relu Activation Function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Relu\"\n",
    "        \n",
    "    def fw(self,w,x):\n",
    "        return np.maximum(0,w.T.dot(x))\n",
    "\n",
    "    def bp(self,w,x):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "96e0b1c1-da22-4d31-9563-9c46b5f72c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"MultiLayer Perceptron\n",
    "    Implementation Notes:\n",
    "    - input and output layers must be defined explicitly.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers  = []\n",
    "        self.weights = []\n",
    "\n",
    "    def add_layer(self,\n",
    "                  nodes:int,\n",
    "                  afunc:AbstractAF) -> None:\n",
    "        \"\"\"Adds a layer \"\"\"\n",
    "        self.layers.append(MLPLayer(nodes,afunc))\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights based on added layers\"\"\"\n",
    "        assert len(self.layers) > 1, \"layers must be added\"\n",
    "\n",
    "        # reset weights matrix\n",
    "        self.weights = []\n",
    "\n",
    "        # get the shape based on existing layers\n",
    "        for i in range(1,len(self.layers)):\n",
    "            w_shape = (self.layers[i-1].get_nodes(),\n",
    "                       self.layers[i  ].get_nodes())\n",
    "            self.weights.append(np.zeros(w_shape))\n",
    "\n",
    "    def fw(self,x:np.array):\n",
    "        \"\"\"Performs a forward pass from\n",
    "        x through n hidden layers to f_w(x)\n",
    "        by applying an activation function \n",
    "        for each layer in the MLP.\n",
    "\n",
    "        The function also initializes weight\n",
    "        dimensions, if not done so already.\n",
    "\n",
    "        Given the input example:\n",
    "        x_ample = np.ones((3,n))\n",
    "        \n",
    "        each column would represent a sample\n",
    "        ie: \n",
    "        > x_ample[:,0]   would be the 1st sample\n",
    "        > x_ample[:,1]   would be the 2nd sample\n",
    "        > x_ample[:,n-1] would be the nth sample\n",
    "        etc.\n",
    "        \n",
    "        each row would represent a variable\n",
    "        ie:\n",
    "        > x_ample[0,:] would be the 1st parameter\n",
    "        > x_ample[1,:] would be the 2nd parameter\n",
    "        > x_ample[2,:] would be the 3rd parameter\n",
    "        etc.\n",
    "\n",
    "        The output of this function will generally take the shape:\n",
    "        (m,n) where n is the number of columns in the input array\n",
    "        and m is the number of node is the final layer in this MLP.\n",
    "        In this case, we are predicting one value, how late the\n",
    "        MBTA will be, and therefore m will always be 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # init weights if not yet done\n",
    "        if len(self.weights) == 0:\n",
    "            self._init_weights()\n",
    "\n",
    "        # loop through and update x iteratively:\n",
    "        for i in range(1,len(self.layers)):\n",
    "            x = self.layers[i].fw(self.weights[i-1],x)\n",
    "\n",
    "        # return x\n",
    "        return x # <- TODO double check if i should be storing the h's....\n",
    "\n",
    "    def bp(self,x:np.array,y:np.array):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gd(self,x,y,eta:int=10_000):\n",
    "        # list of errors?\n",
    "        ls = []\n",
    "        \n",
    "        for i in range(eta):\n",
    "            # self.fw(x)\n",
    "            # self.bp(x)\n",
    "            ...\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "90c87d14-0beb-4c19-91a3-1891889c1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer:\n",
    "    \"\"\"Represents a single layer in the MLP.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,nodes,afunc):\n",
    "        self.nodes = int(nodes)\n",
    "        self.afunc = afunc\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"overwrite representation for pretty print\"\"\"\n",
    "        return \"<MLPLayer: {nodes:\"+f\"{self.nodes},afunc:{self.afunc}\"+\"}>\"\n",
    "\n",
    "    def get_nodes(self):\n",
    "        return self.nodes+0\n",
    "\n",
    "    def fw(self,w:np.array,x:np.array):\n",
    "        return self.afunc.fw(w=w,x=x)\n",
    "\n",
    "    def bp(self,w:np.array,x:np.array):\n",
    "        return self.afunc.bp(w=w,x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e27cb-e92f-4d4a-8551-e1f23d4c3a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2164e32-86e7-47bd-b4f2-70a4834232ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65905e58-8679-4835-97b6-e7ca3f722727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d4eabf47-df78-4364-a879-e8ee5b63abd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MLPLayer: {nodes:3,afunc:<ActivationFunction:Linear>}>,\n",
       " <MLPLayer: {nodes:40,afunc:<ActivationFunction:Linear>}>,\n",
       " <MLPLayer: {nodes:80,afunc:<ActivationFunction:Relu>}>,\n",
       " <MLPLayer: {nodes:20,afunc:<ActivationFunction:Relu>}>,\n",
       " <MLPLayer: {nodes:1,afunc:<ActivationFunction:Relu>}>]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp.add_layer(3,LinearAF())  # input x\n",
    "mlp.add_layer(40,LinearAF()) # hidden layer #1\n",
    "mlp.add_layer(80,ReluAF())   # hidden layer #2\n",
    "mlp.add_layer(20,ReluAF())   # hidden layer #3\n",
    "mlp.add_layer(1,ReluAF())    # prediction f_w(x)\n",
    "mlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a38897b6-1378-4c1e-8a6f-2c9ad53c7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 40)\n",
      "(40, 80)\n",
      "(80, 20)\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "mlp._init_weights()\n",
    "for weight_matrix in mlp.weights:\n",
    "    print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "11d7a64c-2d2f-46de-8244-fb5d3d6315b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# example input:\n",
    "x_ample = np.ones((3,5))\n",
    "\n",
    "\"\"\"\n",
    "Given the imput example:\n",
    "x_ample = np.ones((3,5))\n",
    "\n",
    "each column would represent a sample\n",
    "ie: \n",
    "> x_ample[:,0] would be the first weather sample\n",
    "> x_ample[:,1] would be the second weather sample\n",
    "etc.\n",
    "\n",
    "each row would represent a variable\n",
    "ie:\n",
    "> x_ample[1,:] would be all the tempertures\n",
    "> x_ample[2,:] would be all the precipitations\n",
    "> x_ample[3,:] would be all the biases\n",
    "etc.\n",
    "\"\"\"\n",
    "\n",
    "# going to print an instance of a forward pass\n",
    "print(\n",
    "    mlp.fw(x_ample)\n",
    ")\n",
    "\"\"\"\n",
    "mlp.fw(x_ample)\n",
    "is a matrix that looks like this:\n",
    "np.array([[0,0,0,0,0]]) # for now:\n",
    "\n",
    "again, each column would represent\n",
    "a prediction of y.\n",
    "\n",
    "If we had multiple rows, as an output,\n",
    "it could / would be displayed here.\n",
    "and we could do some sort of \n",
    "verification with those.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d82f9-596a-4cb2-8e26-23a25a64be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16951a-86e9-4f67-8768-64b3474f2398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
