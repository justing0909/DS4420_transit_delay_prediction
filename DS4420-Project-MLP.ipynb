{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d023b41-3653-40d9-a995-da2ee35e1d66",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e32326-cd14-4daf-8e36-e608110af237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f00c6-7dac-46f3-9387-de1d70243bf5",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174f379-57b8-4117-ba37-fa94410480f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596ea43-3342-4750-8021-261a1cb60f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58070d89-2653-415d-9963-c746d10f9717",
   "metadata": {},
   "source": [
    "# pointers\n",
    "(set pointers for x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8aa3c02-26d3-4a78-90a1-907f5b342a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite when ready.....\n",
    "x_train = np.ones((10,3))\n",
    "x_test  = np.ones((5,3))\n",
    "y_train = np.ones((10,1))\n",
    "y_test  = np.ones((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250e9003-e5a3-4276-8bc2-e8ea4de8cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.ones((3,10))\n",
    "# y = np.ones((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07397f1-682d-423c-ab50-3a64de5f3b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9,  0],\n",
       "       [12,  1, -3, 22,  3, -1,  2, 31, 31,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "     1, 2, 3, 4, 5, 6, 7, 8, 9, 0,\n",
    "    12, 1,-3,22, 3,-1, 2,31,31, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "]).reshape(3,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4754c3-eb24-4b98-8d21-513e11becd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73479592,  3.80050051,  6.01862263,  7.87977243,  9.68051088,\n",
       "        11.75259277, 13.68534001, 15.83761103, 17.70600538, -0.29470625]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2*x[0,:] + np.random.normal(0,0.25,10).reshape(1,-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea9414-17ee-4091-9b5c-8f3f8225f902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38131b9d-305c-4b2d-b3b5-2d9d6a84f423",
   "metadata": {},
   "source": [
    "# define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2705,
   "id": "fd28f343-ffdc-4027-9fc8-43eb3f4a4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAF:\n",
    "    \"\"\"Abstract Activation Function.\n",
    "    This is an abstract class used to represent\n",
    "    activation functions in a MultiLayer Perceptitron.\n",
    "\n",
    "    Each Activation function must have a name\n",
    "    and implement the following three functions:\n",
    "    - fw(w,x)         represents a forward pass through the MLP\n",
    "    - bp_w(w,x)       represents a dL/dw backprop through the MLP\n",
    "    - bp_x(w,x)       represents a dL/dh backprop through the MLP <- TODO look into this.....\n",
    "\n",
    "    This class does not implement any of the three functions.\n",
    "    Child-classes MUST implement all three functions for \n",
    "    backprop to work properly.\n",
    "\n",
    "    In the current implementation, the following classes are the only valid subclasses:\n",
    "    - LinearAF\n",
    "    - ReluAF\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Abstract\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Overwrites the representation with class name.\n",
    "        This function makes the print look cleaner :) \n",
    "        \"\"\"\n",
    "        return f\"<ActivationFunction:{self.name}>\"\n",
    "    \n",
    "    def fw(self,w,x):\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "    def bp_w(self,w,x):\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "    def bp_x(self,w,x):\n",
    "        raise NotImplementedError(\"Abstract Class cannot run functions.  Please use a subclass.\")\n",
    "\n",
    "class MeanSquaredErrorAF(AbstractAF):\n",
    "    \"\"\"Mean Squared Error function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"MSE\"\n",
    "        self.axis = 0\n",
    "\n",
    "    def fw(self,f,y):\n",
    "        return   np.mean((f-y)**2,axis=self.axis)\n",
    "\n",
    "    def bp(self,f,y):\n",
    "        return 2*np.mean((f-y),   axis=self.axis)\n",
    "\n",
    "class LinearAF(AbstractAF):\n",
    "    \"\"\"Linear Activation Function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Linear\"\n",
    "    \n",
    "    def fw(self,w,x):\n",
    "        return w.T.dot(x)\n",
    "\n",
    "    def bp_w(self,w,x):\n",
    "        return x\n",
    "\n",
    "    def bp_x(self,w,x):\n",
    "        return w\n",
    "\n",
    "class ReluAF(AbstractAF):\n",
    "    \"\"\"Relu Activation Function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Relu\"\n",
    "        \n",
    "    def fw(self,w,x):\n",
    "        return np.maximum(0,w.T.dot(x))\n",
    "\n",
    "    def bp_w(self,w,x):\n",
    "        print(\"wtx:\",(w.T.dot(x) > 0).shape)\n",
    "        print(\"x:\",x.shape,\"(expected)\")\n",
    "        return x.dot((w.T.dot(x) > 0).T)\n",
    "\n",
    "    def bp_x(self,w,x):\n",
    "        print(\"wtx:\",(w.T.dot(x) > 0).shape)\n",
    "        print(\"w:\",w.shape,\"(expected)\")\n",
    "        return (w).dot(w.T.dot(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2742,
   "id": "96e0b1c1-da22-4d31-9563-9c46b5f72c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"MultiLayer Perceptron\n",
    "    Implementation Notes:\n",
    "    - input and output layers must be defined explicitly.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers  = []\n",
    "        self.weights = []\n",
    "        self.loss = MeanSquaredErrorAF()\n",
    "\n",
    "    def add_layer(self,nodes:int,afunc:AbstractAF) -> None:\n",
    "        \"\"\"Adds a layer with a given number of nodes\n",
    "        and a given Abstract Function\"\"\"\n",
    "        self.layers.append(MLPLayer(nodes,afunc))\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights based on added layers\"\"\"\n",
    "        assert len(self.layers) > 1, \"layers must be added\"\n",
    "\n",
    "        # reset weights matrix\n",
    "        self.weights = []\n",
    "\n",
    "        # get the shape based on existing layers\n",
    "        for i in range(1,len(self.layers)):\n",
    "            w_shape = (self.layers[i-1].get_nodes(),\n",
    "                       self.layers[i  ].get_nodes())\n",
    "            self.weights.append(np.ones(w_shape)*0.01)\n",
    "\n",
    "    def fw(self,x:np.array):\n",
    "        \"\"\"Performs a forward pass from\n",
    "        x through n hidden layers to f_w(x)\n",
    "        by applying an activation function \n",
    "        for each layer in the MLP.\n",
    "\n",
    "        The function also initializes weight\n",
    "        dimensions, if not done so already.\n",
    "\n",
    "        Given the input example:\n",
    "        x_ample = np.ones((3,n))\n",
    "        \n",
    "        each column would represent a sample\n",
    "        ie: \n",
    "        > x_ample[:,0]   would be the 1st sample\n",
    "        > x_ample[:,1]   would be the 2nd sample\n",
    "        > x_ample[:,n-1] would be the nth sample\n",
    "        etc.\n",
    "        \n",
    "        each row would represent a variable\n",
    "        ie:\n",
    "        > x_ample[0,:] would be the 1st parameter\n",
    "        > x_ample[1,:] would be the 2nd parameter\n",
    "        > x_ample[2,:] would be the 3rd parameter\n",
    "        etc.\n",
    "\n",
    "        The output of this function will generally take the shape:\n",
    "        (m,n) where n is the number of columns in the input array\n",
    "        and m is the number of node is the final layer in this MLP.\n",
    "        In this case, we are predicting one value, how late the\n",
    "        MBTA will be, and therefore m will always be 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # init weights if not yet done\n",
    "        if len(self.weights) == 0:\n",
    "            self._init_weights()\n",
    "\n",
    "        # initialize x as the hidden value\n",
    "        # of layer 0 (the input layer)\n",
    "        self.layers[0].h = x\n",
    "\n",
    "        # loop through and update x iteratively:\n",
    "        for i in range(1,len(self.layers)):\n",
    "            x = self.layers[i].fw(self.weights[i-1],x)\n",
    "\n",
    "        # return x\n",
    "        return x\n",
    "    \n",
    "    def _bp_list_factors(self,ridx,debug:bool=False):\n",
    "        \"\"\"Gets a list of factors to\n",
    "        generate the corresponding\n",
    "        weight matrix.\n",
    "        \n",
    "        ridx is the reversed index:\n",
    "        - 0 refers to the last element\n",
    "        - 1 refers to the 2nd to last element\n",
    "        etc.\n",
    "        \"\"\"\n",
    "        reversed_weights = list(reversed(self.weights))\n",
    "        reversed_layers  = list(reversed(self.layers))\n",
    "\n",
    "        # store factors to prod later \n",
    "        prod_factors = []\n",
    "\n",
    "        # loop through the layers add dh\n",
    "        for i in range(ridx):\n",
    "            if debug:\n",
    "                print(f\"\"\"iteration:[{i}]:\\n\n",
    "                layer.h: {reversed_layers[i+1].h.shape}\\n\n",
    "                weight : {reversed_weights[i].shape}\\n\n",
    "                dotable: {...}\\n\n",
    "                \"\"\")\n",
    "            \n",
    "            # print(f\"{reversed_layers[i+1]}.bp_x(...); shape:{reversed_weights[i].shape}\")\n",
    "            prod_factors.append(reversed_layers[i+1].bp_x(reversed_weights[i]))\n",
    "\n",
    "        # add dw\n",
    "        # print(f\"{reversed_layers[ridx+1]}.bp_w(...); shape:{reversed_weights[ridx].shape}\")\n",
    "        prod_factors.append(reversed_layers[ridx+1].bp_w(reversed_weights[ridx]))\n",
    "\n",
    "        # return factors\n",
    "        return prod_factors\n",
    "\n",
    "    def _bp_dot(self,bp_list,loss,debug:bool=False):\n",
    "        \"\"\"bp_list is the list generated from _bp_list_factors()\n",
    "        loss is the VALUES of loss as a matrix\n",
    "        \"\"\"\n",
    "        prod_dh = loss.copy()\n",
    "    \n",
    "        # ignore the last value b/c it's dw not dh\n",
    "        for i in range(len(bp_list) - 1):\n",
    "            # perform a cumulative dot product\n",
    "            # starting from back:\n",
    "            if debug:\n",
    "                print(f\"\"\"iteration:[{i}]:\\n\n",
    "                bp_list: {bp_list[i].shape}\\n\n",
    "                prod_dh: {prod_dh.shape}\\n\n",
    "                dotable: {bp_list[i][1]==prod_dh.shape[0]}\\n\n",
    "                \"\"\")\n",
    "                \n",
    "            try:\n",
    "                prod_dh = bp_list[i].dot(prod_dh)\n",
    "            except:\n",
    "                prod_dh = bp_list[i] * (prod_dh)\n",
    "            \n",
    "    \n",
    "        # dot dw with the prod_dh transpose\n",
    "        dldw = bp_list[-1].dot(prod_dh.T)\n",
    "        return dldw\n",
    "    \n",
    "    def gd(self,\n",
    "           x:np.array,\n",
    "           y:np.array,\n",
    "           eta:float=0.1,\n",
    "           iters:int=10,\n",
    "           debug:bool=False\n",
    "          ):\n",
    "        # list of errors?\n",
    "        ls_mse = []\n",
    "        \n",
    "        for i in range(iters):\n",
    "            # compute the fwd pass\n",
    "            fwp = self.fw(x)\n",
    "            # compute the loss\n",
    "            fwl = self.loss.fw(f=fwp,y=y).reshape(1,-1)\n",
    "            bpl = self.loss.bp(f=fwp,y=y).reshape(1,-1)\n",
    "            for fidx in range(len(self.weights)):\n",
    "                ridx = len(self.weights) - fidx - 1\n",
    "                bpd = self._bp_dot(self._bp_list_factors(fidx),bpl,debug=debug)\n",
    "                    \n",
    "                if debug:\n",
    "                    print(f\"shape match: {self.weights[ridx].shape == bpd.shape}\")\n",
    "                    print(f\"    self.weights[{ridx}]\",self.weights[ridx].shape)\n",
    "                    print(f\"    self._bp_dot[{ridx}]\",bpd.shape)\n",
    "\n",
    "                if bpd.shape == self.weights[ridx].shape:\n",
    "                    # overwrite the weights if the shapes match:\n",
    "                    self.weights[ridx] = (self.weights[ridx] - eta * bpd)\n",
    "                else:\n",
    "                    # throw error otherwise\n",
    "                    raise Exception(\"invalid weight shape\"+\n",
    "                                    f\"expected{self.weights[ridx].shape}; got{bpd.shape}\")\n",
    "            \n",
    "            ls_mse.append(fwl)\n",
    "        return ls_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2744,
   "id": "90c87d14-0beb-4c19-91a3-1891889c1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer:\n",
    "    \"\"\"Represents a single layer in the MLP.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,nodes,afunc):\n",
    "        self.nodes = int(nodes)\n",
    "        self.afunc = afunc\n",
    "        self.h = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"overwrite representation for pretty print\"\"\"\n",
    "        return \"<MLPLayer: {nodes:\"+f\"{self.nodes},afunc:{self.afunc}\"+\"}>\"\n",
    "\n",
    "    def get_nodes(self):\n",
    "        return self.nodes+0\n",
    "\n",
    "    def fw(self,w:np.array,x:np.array):\n",
    "        \"\"\"store and return the\n",
    "        post-activation values \n",
    "        of a forward pass.\"\"\"\n",
    "        self.h = self.afunc.fw(w=w,x=x)\n",
    "        return self.h.copy()\n",
    "\n",
    "    def bp_w(self,w:np.array):\n",
    "        return self.afunc.bp_w(w=w,x=self.h)\n",
    "\n",
    "    def bp_x(self,w:np.array):\n",
    "        return self.afunc.bp_x(w=w,x=self.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e27cb-e92f-4d4a-8551-e1f23d4c3a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2747,
   "id": "af2dd9e7-7209-4116-a737-e55420c2ef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9,  0],\n",
       "       [12,  1, -3, 22,  3, -1,  2, 31, 31,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 2747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2749,
   "id": "a66ecf29-8de4-40cc-9b4c-a93c92b0e4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73479592,  3.80050051,  6.01862263,  7.87977243,  9.68051088,\n",
       "        11.75259277, 13.68534001, 15.83761103, 17.70600538, -0.29470625]])"
      ]
     },
     "execution_count": 2749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2751,
   "id": "a5871c55-7f64-4b59-ab2c-2966f756462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertweights(mlp):\n",
    "    assert mlp.weights[0].shape == (3,4)\n",
    "    assert mlp.weights[1].shape == (4,5)\n",
    "    assert mlp.weights[2].shape == (5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2753,
   "id": "9e46e966-db94-4254-8eed-2372d7a63f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertlayers(mlp):\n",
    "    assert mlp.layers[0].h.shape == (3,10)\n",
    "    assert mlp.layers[1].h.shape == (4,10)\n",
    "    assert mlp.layers[2].h.shape == (5,10)\n",
    "    assert mlp.layers[3].h.shape == (1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2755,
   "id": "51cffb62-7b04-44ab-9b89-c9c1cec3cd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape match: True\n",
      "    self.weights[2] (5, 1)\n",
      "    self._bp_dot[2] (5, 1)\n",
      "iteration:[0]:\n",
      "\n",
      "                bp_list: (5, 1)\n",
      "\n",
      "                prod_dh: (1, 10)\n",
      "\n",
      "                dotable: [False]\n",
      "\n",
      "                \n",
      "shape match: True\n",
      "    self.weights[1] (4, 5)\n",
      "    self._bp_dot[1] (4, 5)\n",
      "iteration:[0]:\n",
      "\n",
      "                bp_list: (5, 1)\n",
      "\n",
      "                prod_dh: (1, 10)\n",
      "\n",
      "                dotable: [False]\n",
      "\n",
      "                \n",
      "iteration:[1]:\n",
      "\n",
      "                bp_list: (4, 5)\n",
      "\n",
      "                prod_dh: (5, 10)\n",
      "\n",
      "                dotable: [False False False False False]\n",
      "\n",
      "                \n",
      "shape match: True\n",
      "    self.weights[0] (3, 4)\n",
      "    self._bp_dot[0] (3, 4)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp.add_layer(3,LinearAF()) # input x\n",
    "mlp.add_layer(4,LinearAF())   # hidden layer #1\n",
    "mlp.add_layer(5,LinearAF())   # hidden layer #2\n",
    "mlp.add_layer(1,ReluAF())   # prediction f_w(x)\n",
    "mlp._init_weights()\n",
    "\n",
    "# assert weight shape\n",
    "assertweights(mlp)\n",
    "\n",
    "# run fw pass and assert shapes:\n",
    "mlp.fw(x)\n",
    "\n",
    "# assert weights / matricies shape\n",
    "assertweights(mlp)\n",
    "assertlayers(mlp)\n",
    "\n",
    "# print(\"# product list for w2\")\n",
    "# bplf0 = mlp._bp_list_factors(0)\n",
    "# assert len(bplf0) == 1\n",
    "# assert bplf0[0].shape == (5,10), \\\n",
    "# f\"\"\"expected: {(5,10)}; got: {bplf0[0].shape}\\n\"\"\"   # takes shape of h\n",
    "\n",
    "# print(\"# product list for w1\")\n",
    "# bplf1 = mlp._bp_list_factors(1)\n",
    "# assert len(bplf1) == 2\n",
    "# assert bplf1[0].shape == (5,1),  \\\n",
    "# f\"\"\"expected: {(5,1)};  got: { bplf1[0].shape}\\n\"\"\"  # takes shape of w\n",
    "# assert bplf1[1].shape == (4,10), \\\n",
    "# f\"\"\"expected: {(4,10)}; got: { bplf1[1].shape}\\n\"\"\"  # takes shape of h\n",
    "\n",
    "# print(\"# product list for w0\")\n",
    "# bplf2 = mlp._bp_list_factors(2)\n",
    "# assert len(bplf2) == 3\n",
    "# assert bplf2[0].shape == (5,1),  \\\n",
    "# f\"\"\"expected: {(5,1)};  got: { bplf2[0].shape}\\n\"\"\"  # takes shape of w\n",
    "# assert bplf2[1].shape == (4,5),  \\\n",
    "# f\"\"\"expected: {(4,1)};  got: { bplf2[1].shape}\\n\"\"\"  # takes shape of w\n",
    "# assert bplf2[2].shape == (3,10), \\\n",
    "# f\"\"\"expected: {(3,10)}; got: { bplf2[2].shape}\\n\"\"\"  # takes shape of h\n",
    "\n",
    "mlp.gd(x,y,eta=0.00002,iters=1,debug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74cdab6-ff5e-404d-ac3c-320523c85225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2757,
   "id": "7437b82d-f1fe-4856-a27c-54249191bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 2757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp._bp_list_factors(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2759,
   "id": "1ad8ad21-2a93-4342-83c2-7e7b29ee5a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 2759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp._bp_list_factors(1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d73bb9-52b2-41c4-8da7-88383b5ee75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2762,
   "id": "92ea327e-7872-400f-b3be-265d5c045c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.gd(x,y,eta=0.0002,iters=1000,debug=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2764,
   "id": "5e97ec6a-97b8-40f0-bf98-0ce153661d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.67337392, 29.49631076,  0.        ]])"
      ]
     },
     "execution_count": 2764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.array([10,15,-1,\n",
    "                   22,-1, 3,\n",
    "                    1, 1, 1,]).reshape(3,-1)\n",
    "# expect y_test to be ~20, ~30, -2\n",
    "\n",
    "mlp.fw(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a5d13-787a-4a87-a8f1-593fab789e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2767,
   "id": "205442f4-ad85-41bd-9683-cbf9961b6369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.68199078e-01,  8.68199078e-01,  8.68199078e-01,\n",
       "          8.68199078e-01],\n",
       "        [ 5.83538476e-04,  5.83538476e-04,  5.83538476e-04,\n",
       "          5.83538476e-04],\n",
       "        [-2.75651652e-02, -2.75651652e-02, -2.75651652e-02,\n",
       "         -2.75651652e-02]]),\n",
       " array([[0.27259008, 0.27259008, 0.27259008, 0.27259008, 0.27259008],\n",
       "        [0.27259008, 0.27259008, 0.27259008, 0.27259008, 0.27259008],\n",
       "        [0.27259008, 0.27259008, 0.27259008, 0.27259008, 0.27259008],\n",
       "        [0.27259008, 0.27259008, 0.27259008, 0.27259008, 0.27259008]]),\n",
       " array([[0.41634832],\n",
       "        [0.41634832],\n",
       "        [0.41634832],\n",
       "        [0.41634832],\n",
       "        [0.41634832]])]"
      ]
     },
     "execution_count": 2767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90cc80-5d0f-4864-a29e-a3aafe07628f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360ebd7-9eba-465e-96bc-078f619d08a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437b7ac-b8ee-47c8-8526-76bd1b51accd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910e1e0-ecb0-4384-811d-5cd68f48ef62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6c83d-d8e2-41ef-820d-145f95fe971c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c686e1-f8a7-491e-bbdd-17aadd29d536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f5c49-ccc5-47c6-865c-2e9e8a116ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278b8b7-9ca4-401f-8d09-c75bffcb7d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7a64c-2d2f-46de-8244-fb5d3d6315b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input:\n",
    "x_ample = np.ones((3,5))\n",
    "\n",
    "\"\"\"\n",
    "Given the imput example:\n",
    "x_ample = np.ones((3,5))\n",
    "\n",
    "each column would represent a sample\n",
    "ie: \n",
    "> x_ample[:,0] would be the first weather sample\n",
    "> x_ample[:,1] would be the second weather sample\n",
    "etc.\n",
    "\n",
    "each row would represent a variable\n",
    "ie:\n",
    "> x_ample[1,:] would be all the tempertures\n",
    "> x_ample[2,:] would be all the precipitations\n",
    "> x_ample[3,:] would be all the biases\n",
    "etc.\n",
    "\"\"\"\n",
    "\n",
    "# going to print an instance of a forward pass\n",
    "print(\n",
    "    mlp.fw(x_ample)\n",
    ")\n",
    "\"\"\"\n",
    "mlp.fw(x_ample)\n",
    "is a matrix that looks like this:\n",
    "np.array([[0,0,0,0,0]]) # for now:\n",
    "\n",
    "again, each column would represent\n",
    "a prediction of y.\n",
    "\n",
    "If we had multiple rows, as an output,\n",
    "it could / would be displayed here.\n",
    "and we could do some sort of \n",
    "verification with those.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d82f9-596a-4cb2-8e26-23a25a64be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9d283-a3c6-4f3c-83c7-87d884867573",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layers = mlp.layers.copy()\n",
    "test_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d89e8-4bf2-4d8c-9dd8-88fc34c7ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights = mlp.weights.copy()\n",
    "test_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d3cd5-42e9-4489-a1f8-33cf342f8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bp(ridx):\n",
    "    \"\"\"ridx is the reversed index:\n",
    "    - 0 refers to the last element\n",
    "    - 1 refers to the 2nd to last element\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    reversed_weights = list(reversed(test_weights))\n",
    "    reversed_layers  = list(reversed(test_weights))\n",
    "    \n",
    "    prod_factors = []\n",
    "    \n",
    "    for i in range(ridx):\n",
    "        prod_factors.append(f\"reversed_layers[{i}].bp_x(...)\")\n",
    "\n",
    "    prod_factors.append(f\"reversed_layers[{ridx}].bp_w(...)\")\n",
    "    return prod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405560f2-1a32-4c8d-9608-89ef6a75a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e79d6-492c-495c-9f55-1cbfab8a69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b034d-abc8-4e36-b253-a6bbaf51bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef240b-1444-4978-a78b-8102143a8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a0077-f670-430d-a6eb-399ed6a332d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a84cd8-4cdd-4260-8750-cc2a892c1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reversed(test_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773b123-bfa9-43b5-b491-d98d43b53a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16951a-86e9-4f67-8768-64b3474f2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16ff88-989c-4445-89f7-e875961fce52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638a579-5966-4a06-95bf-928e518984c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(10-1-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2685,
   "id": "050e4343-28dd-415c-ad86-0f8753d18a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0026, 0.0006, 0.    , 0.0052, 0.0016, 0.001 , 0.0018, 0.0078,\n",
       "        0.008 , 0.    ]])"
      ]
     },
     "execution_count": 2685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp.add_layer(2,LinearAF()) # input x\n",
    "mlp.add_layer(2,ReluAF())   # hidden layer #1\n",
    "mlp.add_layer(1,LinearAF())   # prediction f_w(x)\n",
    "mlp._init_weights()\n",
    "\n",
    "# run fw pass and assert shapes:\n",
    "mlp.fw(x[:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2687,
   "id": "0d02f25a-fbbf-4844-a48d-a1a8dd5b28bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.01, 0.01],\n",
       "        [0.01, 0.01]]),\n",
       " array([[0.01],\n",
       "        [0.01]])]"
      ]
     },
     "execution_count": 2687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2689,
   "id": "3ca796f5-9891-4d0c-9d66-c867db4cea26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MLPLayer: {nodes:2,afunc:<ActivationFunction:Linear>}>,\n",
       " <MLPLayer: {nodes:2,afunc:<ActivationFunction:Relu>}>,\n",
       " <MLPLayer: {nodes:1,afunc:<ActivationFunction:Linear>}>]"
      ]
     },
     "execution_count": 2689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2691,
   "id": "8fb11d3a-acc7-45ff-bb25-6c36811d55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_fw_list(idx,loss):\n",
    "    \"\"\"Use the \"forward index\"\n",
    "    to procure the derivative\n",
    "    of a weight.\"\"\"\n",
    "    weight = mlp.weights[idx]\n",
    "    wshape = weight.shape\n",
    "    \n",
    "    # init list and append loss\n",
    "    bp_list = []\n",
    "    bp_list.append(loss) # 1xn\n",
    "    if idx == len(mlp.weights) - 1:\n",
    "        bp_list.append(mlp.layers[idx].h.T)\n",
    "    if idx == len(mlp.weights) - 2:\n",
    "        bp_list.append(mlp.weights[idx])\n",
    "        bp_list.append(mlp.layers[idx-1].h.T)\n",
    "\n",
    "    return bp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2695,
   "id": "0fb16148-8f3a-4c9c-8a26-b0c561ac4adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]]),\n",
       " array([[0.13, 0.13],\n",
       "        [0.03, 0.03],\n",
       "        [0.  , 0.  ],\n",
       "        [0.26, 0.26],\n",
       "        [0.08, 0.08],\n",
       "        [0.05, 0.05],\n",
       "        [0.09, 0.09],\n",
       "        [0.39, 0.39],\n",
       "        [0.4 , 0.4 ],\n",
       "        [0.  , 0.  ]])]"
      ]
     },
     "execution_count": 2695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_fw_list(1,np.ones((1,1))) # get w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2701,
   "id": "86a85123-acd5-4614-97b4-15460a9f9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]]),\n",
       " array([[0.01, 0.01],\n",
       "        [0.01, 0.01]]),\n",
       " array([[0.0026],\n",
       "        [0.0006],\n",
       "        [0.    ],\n",
       "        [0.0052],\n",
       "        [0.0016],\n",
       "        [0.001 ],\n",
       "        [0.0018],\n",
       "        [0.0078],\n",
       "        [0.008 ],\n",
       "        [0.    ]])]"
      ]
     },
     "execution_count": 2701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp0 = bp_fw_list(0,np.ones((1,1))) # get w1\n",
    "bp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f8c16-efa1-46a5-ad46-6177de77334b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b4836-f1a0-4c4f-84fe-dacfc74cf6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
